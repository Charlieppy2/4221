# 快速数据准备指南

## 🎯 快速开始

### 情况1: 您已有训练数据

1. **将图片放入对应类别文件夹**
   ```
   data/raw/identity_card/      ← 放入身份證图片
   data/raw/utility_bill/       ← 放入水電費單图片
   data/raw/bank_statement/     ← 放入銀行賬單图片
   data/raw/address_proof/      ← 放入地址證明图片
   data/raw/lease_agreement/    ← 放入租約图片
   data/raw/other/              ← 放入其他文檔图片
   ```

2. **运行预处理脚本**
   ```bash
   cd model_training
   python data_preprocessing.py
   ```

3. **开始训练**
   ```bash
   python train.py
   ```

### 情况2: 您没有训练数据（快速测试）

如果您想快速测试训练流程，可以使用以下方法：

#### 方法A: 创建示例图片（最小数据集）

1. **手动创建少量示例图片**
   - 使用手机拍摄几张不同类别的文档照片
   - 或使用图片编辑软件创建简单的文档模板图片
   - 每个类别至少准备 5-10 张图片用于测试

2. **将图片放入对应的 `data/raw/类别名/` 文件夹**

3. **运行预处理和训练**
   ```bash
   cd model_training
   python data_preprocessing.py
   python train.py
   ```

#### 方法B: 使用数据增强生成更多样本

如果只有少量原始图片，可以使用数据增强：

1. **准备少量原始图片**（每个类别 10-20 张）

2. **运行数据增强脚本**
   ```python
   # 在 data_preprocessing.py 中
   preprocessor.augment_data(augment_per_image=5)  # 每张生成5个变体
   ```

3. **这样可以将数据集扩大 5-6 倍**

### 情况3: 从公开数据集获取

#### 推荐数据集

1. **Kaggle**
   - 搜索 "document classification"
   - 搜索 "receipt dataset"
   - 搜索 "ID card dataset"

2. **GitHub**
   - 搜索文档识别相关的仓库
   - 查看是否有公开的数据集链接

3. **学术数据集**
   - [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) - 文档图像分类数据集
   - [Tobacco-3482](https://www.kaggle.com/datasets/patrickaudriaz/tobacco3482jpg) - 文档分类数据集

#### 使用步骤

1. **下载数据集**

2. **按类别分类**
   - 根据数据集提供的标签或您的分类标准
   - 将图片分类到对应的文件夹中

3. **运行预处理**

## 📋 数据检查清单

在开始训练前，运行以下命令检查数据：

```bash
cd model_training
python data_preprocessing.py
```

输出会显示：
```
数据分布统计:
--------------------------------------------------
identity_card        :   50 张图片
utility_bill         :   48 张图片
bank_statement       :   52 张图片
address_proof        :   45 张图片
lease_agreement      :   49 张图片
other                :   51 张图片
--------------------------------------------------
總計                 :  295 张图片
```

## ⚠️ 重要提示

### 最小训练数据要求

- **快速测试**: 每个类别至少 **5-10 张**（不推荐用于生产环境）
- **基本训练**: 每个类别至少 **50 张**（可以获得基本效果）
- **正式训练**: 每个类别至少 **100-200 张**（推荐）
- **最佳效果**: 每个类别 **200-500 张**（理想情况）

### 数据不足时的解决方案

1. **数据增强**: 使用 `augment_data()` 生成更多变体
2. **迁移学习**: 使用预训练模型（代码已支持）
3. **合成数据**: 创建文档模板生成更多样本
4. **收集更多数据**: 继续收集真实数据

## 🚀 下一步

数据准备好后：

1. ✅ 运行 `data_preprocessing.py` 检查数据分布
2. ✅ 运行 `train.py` 开始训练模型
3. ✅ 训练完成后运行 `evaluate.py` 评估模型性能

## 💡 提示

- 如果某个类别数据特别少，可以先进行数据增强
- 训练时可以使用较小的 epoch 数进行快速测试（如 10-20 个 epoch）
- 确认数据准备正确后，再运行完整的训练（50 个 epoch）

